{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr\n",
      "Strange\n",
      "loved\n",
      "Pav\n",
      "Bhaji\n",
      "in\n",
      "Mumbai\n",
      "as\n",
      "it\n",
      "costs\n",
      "only\n",
      "2\n",
      "$\n",
      "a\n",
      "plate\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "doc = nlp('Dr Strange loved Pav Bhaji in Mumbai as it costs only 2$ a plate')\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strange loved Pav Bhaji\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "print(doc[1:5])\n",
    "print(type(doc[1:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name        BirthDay        Email\\n',\n",
       " 'Virat       5 June, 1992    virat@kohli.com\\n',\n",
       " 'Maria       12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena      24 June, 1998   serena@williams.com\\n',\n",
       " 'Joe         1 May, 1897     joe@rogan.com']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('list.txt') as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name        BirthDay        Email\\n Virat       5 June, 1992    virat@kohli.com\\n Maria       12 April, 2001  maria@sharapova.com\\n Serena      24 June, 1998   serena@williams.com\\n Joe         1 May, 1897     joe@rogan.com'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['virat@kohli.com', 'maria@sharapova.com', 'serena@williams.com', 'joe@rogan.com']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "print(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " '       ',\n",
       " 'BirthDay',\n",
       " '       ',\n",
       " 'Email',\n",
       " '\\n ',\n",
       " 'Virat',\n",
       " '      ',\n",
       " '5',\n",
       " 'June',\n",
       " ',',\n",
       " '1992',\n",
       " '   ',\n",
       " 'virat@kohli.com',\n",
       " '\\n ',\n",
       " 'Maria',\n",
       " '      ',\n",
       " '12',\n",
       " 'April',\n",
       " ',',\n",
       " '2001',\n",
       " ' ',\n",
       " 'maria@sharapova.com',\n",
       " '\\n ',\n",
       " 'Serena',\n",
       " '     ',\n",
       " '24',\n",
       " 'June',\n",
       " ',',\n",
       " '1998',\n",
       " '  ',\n",
       " 'serena@williams.com',\n",
       " '\\n ',\n",
       " 'Joe',\n",
       " '        ',\n",
       " '1',\n",
       " 'May',\n",
       " ',',\n",
       " '1897',\n",
       " '    ',\n",
       " 'joe@rogan.com']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['give',\n",
       " 'me',\n",
       " 'a',\n",
       " 'double',\n",
       " 'cheeseburger',\n",
       " 'with',\n",
       " 'extra',\n",
       " 'healthy',\n",
       " 'pizza',\n",
       " 'and',\n",
       " 'a',\n",
       " 'diet',\n",
       " 'coke']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#handling special cases\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case('gimme',[\n",
    "    {ORTH: 'gim'},\n",
    "    {ORTH: 'me'}\n",
    "])\n",
    "\n",
    "doc = nlp('give me a double cheeseburger with extra healthy pizza and a diet coke')\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E007] 'sentencizer' already exists in pipeline. Existing names: ['sentencizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#adding a pipeline component manually\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentencizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\PythonMachineLearning\\lib\\site-packages\\spacy\\language.py:810\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    808\u001b[0m name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m factory_name\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names:\n\u001b[1;32m--> 810\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE007\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, opts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names))\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# Overriding pipe name in the config is not supported and will be ignored.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n",
      "\u001b[1;31mValueError\u001b[0m: [E007] 'sentencizer' already exists in pipeline. Existing names: ['sentencizer']"
     ]
    }
   ],
   "source": [
    "#adding a pipeline component manually\n",
    "\n",
    "#nlp.add_pipe('sentencizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange is kinda Strange.\n",
      "Hulk is a big green dude\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange is kinda Strange. Hulk is a big green dude\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "nlp.remove_pipe('sentencizer')\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[,\n",
       " Look,\n",
       " for,\n",
       " data,\n",
       " to,\n",
       " help,\n",
       " you,\n",
       " address,\n",
       " the,\n",
       " question,\n",
       " .,\n",
       " Governments,\n",
       " are,\n",
       " good,\n",
       " ,\n",
       " sources,\n",
       " because,\n",
       " data,\n",
       " from,\n",
       " public,\n",
       " research,\n",
       " is,\n",
       " often,\n",
       " freely,\n",
       " available,\n",
       " .,\n",
       " Good,\n",
       " ,\n",
       " places,\n",
       " to,\n",
       " start,\n",
       " include,\n",
       " http://www.data.gov/,\n",
       " ,,\n",
       " and,\n",
       " http://www.science,\n",
       " .,\n",
       " ,\n",
       " gov/,\n",
       " ,,\n",
       " and,\n",
       " in,\n",
       " the,\n",
       " United,\n",
       " Kingdom,\n",
       " ,,\n",
       " http://data.gov.uk/.,\n",
       " ,\n",
       " Two,\n",
       " of,\n",
       " my,\n",
       " favorite,\n",
       " data,\n",
       " sets,\n",
       " are,\n",
       " the,\n",
       " General,\n",
       " Social,\n",
       " Survey,\n",
       " at,\n",
       " http://www3.norc.org/gss+website/,\n",
       " ,,\n",
       " ,\n",
       " and,\n",
       " the,\n",
       " European,\n",
       " Social,\n",
       " Survey,\n",
       " at,\n",
       " http://www.europeansocialsurvey.org/.,\n",
       " ]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "tokens = [token for token in doc]\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = []\n",
    "for token in tokens:\n",
    "    if token.like_url:\n",
    "        urls.append(token.text)\n",
    "urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony gave two $ to Peter, Bruce gave 500$ to Steve\n"
     ]
    }
   ],
   "source": [
    "text = 'Tony gave two $ to Peter, Bruce gave 500 $ to Steve'\n",
    "doc = nlp(text)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 $\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    if token.like_num and doc[token.i+1].is_currency:\n",
    "        print(token.text , doc[token.i+1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonMachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
