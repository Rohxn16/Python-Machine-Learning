{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c716e26f-38dc-4a28-b138-490a6c7b49d4",
   "metadata": {},
   "source": [
    "# Multiple Variable Linear Regression \n",
    "#### Original Model : f_wb(x) = wx + b | Where x is the size of the land the house is on\n",
    "#### Multivariate linear regression has multiple features or x values represented by x[j] and the ith value of x[j] will be represented by x[j][i]\n",
    "\n",
    "f<sub>w,b</sub>(x) = wx + b (Original Linear Model) <br>\n",
    "f<sub>w,b</sub>(x) = w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + w<sub>3</sub>x<sub>3</sub> + .... + w<sub>n</sub>x<sub>n</sub> + b (Multivariate Model) Where x<sub>j</sub> is a different feature like number of bedrooms, area, bathroom size, AC or not, etc.\n",
    "\n",
    "For a model with n notations we can list the parameters of w<sub>1</sub>,w<sub>2</sub>,w<sub>3</sub> ... w<sub>n</sub> <br>\n",
    "\n",
    "**w** vector or $\\vec{w}$ = [w<sub>1</sub>, w<sub>2</sub>,x<sub>3</sub>...w<sub>n</sub>]<br>\n",
    "**x** vector or $\\vec{x}$ = [x<sub>1</sub>, x<sub>2</sub>,x<sub>3</sub>...x<sub>n</sub>]<br>\n",
    "b is a simple scalar <br>\n",
    "### Where $\\vec{w}$ and $\\vec{x}$ are row vectors representing the collection of all values of w and x\n",
    "f<sub>$\\vec{w}$,b</sub>($\\vec{x}$) = $\\vec{w}$.$\\vec{x}$ + b = w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + w<sub>3</sub>x<sub>3</sub> + .... + w<sub>n</sub>x<sub>n</sub> + b \n",
    "### The above notation is the dot product notation of the multiple linear regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fa9a4f-de90-4b2e-9e18-06ad435709ee",
   "metadata": {},
   "source": [
    "# Vectorization:\n",
    "for $\\vec{w}$ = [$w_1$, $w_2$,$w_3$] <br>\n",
    "and $\\vec{x}$ = [$x_1$, $x_2$,$x_3$]\n",
    "and b be a scalar.\n",
    "\n",
    "f $_w$,$_b$($\\vec{x}$) = ($\\sum_{i=1}^{n}$) + b\n",
    "\n",
    "```python\n",
    "\n",
    "w = np.array([1,2,3])\n",
    "b = 4\n",
    "x = np.array([10,20,30])\n",
    "\n",
    "# without vectorization\n",
    "f = w[0] * x[0] +\n",
    "    w[1] + x[1] + \n",
    "    w[2] + x[2] + b\n",
    "\n",
    "#loop representation\n",
    "\n",
    "f = 0\n",
    "for j in range (0,n):\n",
    "    f = f + w[j] * x[j]\n",
    "f += b\n",
    "\n",
    "#Vectorized multiplication in numpy\n",
    "f = np.dot(w,x) + b\n",
    "    \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6b659-2ca4-4c39-9406-225f8f03ae8c",
   "metadata": {},
   "source": [
    "So, vectorization provides a large speed up in this example. This is because NumPy makes better use of available data parallelism in the underlying hardware. GPU's and modern CPU's implement Single Instruction, Multiple Data (SIMD) pipelines allowing multiple operations to be issued in parallel. This is critical in Machine Learning where the data sets are often very large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a028bb7b-cb7b-4b23-b978-f8d233ddacb0",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression implementation with Gradient Descent using vectorization\n",
    "\n",
    "Model : f$_\\vec{w},_b(\\vec{x})$ = $ w_1x_1 + w_2x_2 + .... + w_nx_n$ = $\\vec{w}.\\vec{x} $+b <br>\n",
    "Cost Function : J($w_1,w_2...w_n$) =  <br>\n",
    "Gradient Descent: { <br>\n",
    "    Repeat:<br>\n",
    "        $w_j = w_j - \\alpha \\frac{\\partial}{\\partial w_j}J(\\vec{w},b) $<br>\n",
    "        $b = b - \\alpha \\frac{\\partial}{\\partial w_b}J(\\vec{w},b) $<br>\n",
    "}\n",
    "\n",
    "## Normal Equation : This is an alternative to gradient descent that can be implemented to find w and b without any iterations but is limited only to linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6868d03d-2f10-4e29-a64a-8d913cdb8075",
   "metadata": {},
   "source": [
    "[Link to Notebook for Detailed explanation and implementation of Multiple Linear regression](https://www.coursera.org/learn/machine-learning/ungradedLab/7GEJh/optional-lab-multiple-linear-regression/lab?path=%2Fnotebooks%2FC1_W2_Lab02_Multiple_Variable_Soln.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff87b16f-da7b-4e33-8cf5-88f5486d1c98",
   "metadata": {},
   "source": [
    "# Feature Scaling:\n",
    "### The work of normalizing data i.e. fitting it within a certain range so that drastically different values may fall under a close range is called feature scaling.\n",
    "\n",
    "There are serveral methods of feature scaling:\n",
    "- Mean Normalizaion\n",
    "- Standard Deviation Normalizaion\n",
    "### It is standard for the Normalized / Scaled value to be within a small bracket of range like, [-1,1] , [-0.5,0.5] etc. Rescaling needs to be done only when the bracket of max ans min values is too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8561ea38-ec8c-48a5-b252-0f005420e384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4a9085-1f46-4d20-ae0c-1f1e1ec7fd91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
