{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b9fc2d-6225-4227-a45e-5275f6acbd51",
   "metadata": {},
   "source": [
    "# Using the concept of linear regression learnt in the previous task to create a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93afb068-4c3e-4d5c-bbae-fe1004934962",
   "metadata": {},
   "source": [
    "The term 'cost' in this assignment might be a little confusing since the data is housing cost. Here, cost is a measure how well our model is predicting the target price of the house. The term 'price' is used for housing data.\n",
    "\n",
    "The equation for cost with one variable is:\n",
    "  $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1}$$ \n",
    " \n",
    "where \n",
    "  $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{2}$$\n",
    "  \n",
    "- $f_{w,b}(x^{(i)})$ is our prediction for data in the training dataset of index $i$ using parameters $w,b$.  \n",
    "- $(f_{w,b}(x^{(i)}) -y^{(i)})^2$ is the squared difference between the target value and the prediction.   \n",
    "- These differences are summed over all the $m$ examples and divided by `2m` to produce the cost, $J(w,b)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8b119b5-273f-49b5-87bb-ccbe8da14c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea303276-87ae-474f-9067-9d985bc58f60",
   "metadata": {},
   "source": [
    "# The main goal of linear regression is to minimize J(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b78fcf5f-5ec4-420a-a834-0a1f2fbd67df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x,y,w,b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    \n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    \n",
    "    m = x.shape[0]\n",
    "    \n",
    "    cost_sum = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = (f_wb - y[i]) ** 2\n",
    "        cost_sum = cost_sum + cost\n",
    "    total_cost = (1/(2*m)) * cost_sum\n",
    "    return total_cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b15b676-117a-472d-9c64-e25007db0402",
   "metadata": {},
   "source": [
    " Your goal is to find a model $f_{w,b}(x) = wx + b$, with parameters $w,b$,  which will accurately predict house values given an input $x$. The cost is a measure of how accurate the model is on the training data.\n",
    "\n",
    "The cost equation (1) above shows that if $w$ and $b$ can be selected such that the predictions $f_{w,b}(x)$ match the target data $y$, the $(f_{w,b}(x^{(i)}) - y^{(i)})^2 $ term will be zero and the cost minimized. In this simple two point example, you can achieve this!\n",
    "\n",
    "In the previous lab, you determined that $b=100$ provided an optimal solution so let's set $b$ to 100 and focus on $w$.\n",
    "\n",
    "<br/>\n",
    "Below, use the slider control to select the value of $w$ that minimizes cost. It can take a few seconds for the plot to update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5609be39-f783-4e08-870e-c6cc220dd789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we test our funciton on some live data\n",
    "x_train = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2])\n",
    "y_train = np.array([250, 300, 480,  430,   630, 730,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5d3a299-badd-4e26-9500-c2493f6aad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list = [100,200,300,400,500,600]\n",
    "b_list = [100,150,200,250,300,350]\n",
    "\n",
    "temp_cost = 5600 # entered manually random val\n",
    "w_final = -1\n",
    "b_final = -1\n",
    "all_vals = []\n",
    "for w in w_list:\n",
    "    for b in b_list:\n",
    "        temp = compute_cost(x_train,y_train,w,b)\n",
    "        all_vals.append(temp)\n",
    "        if temp < temp_cost:\n",
    "            temp_cost = temp\n",
    "            w_final = w\n",
    "            b_final = b\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa95f093-70be-4eca-a2d6-a210b9befe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4700.0 is the minimum value of error obtained for the values w = 200 and b = 100 respectively\n"
     ]
    }
   ],
   "source": [
    "# we can have multiple values of w and b and find for which the cost is minimum and use them\n",
    "print(temp_cost,f'is the minimum value of error obtained for the values w = {w_final} and b = {b_final} respectively')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92d9af5d-624c-4cc1-a6c8-ccfa352362b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15933.333333333332, 9850.0, 6266.666666666666, 5183.333333333333, 6600.0, 10516.666666666666, 4700.0, 9783.333333333332, 17366.666666666664, 27450.0, 40033.33333333333, 55116.666666666664, 49100.0, 65350.0, 84100.0, 105350.0, 129100.0, 155350.0, 149133.3333333333, 176550.0, 206466.66666666666, 238883.3333333333, 273800.0, 311216.6666666666, 304800.0, 343383.3333333333, 384466.6666666666, 428050.0, 474133.3333333333, 522716.6666666666, 516100.0, 565850.0, 618100.0, 672850.0, 730100.0, 789850.0]\n"
     ]
    }
   ],
   "source": [
    "print(all_vals) #these are all the error values for different values of w and b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df739249-c0dd-457f-83e3-73f0c5ebfc26",
   "metadata": {},
   "source": [
    "### Now in linear regression, rather than having to manually try to read a contour plot for the best value for w and b, which isn't really a good procedure and also won't work once we get to more complex machine learning models. What you really want is an efficient algorithm that you can write in code for automatically finding the values of parameters w and b they give you the best fit line. That minimizes the cost function j. There is an algorithm for doing this called gradient descent. This algorithm is one of the most important algorithms in machine learning. Gradient descent and variations on gradient descent are used to train, not just linear regression, but some of the biggest and most complex models in all of AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af905d6e-6f7e-48c4-a9ee-9334e462b84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
