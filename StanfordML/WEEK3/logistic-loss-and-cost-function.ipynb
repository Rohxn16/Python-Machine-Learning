{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-04T17:48:45.841598Z","iopub.execute_input":"2024-07-04T17:48:45.842030Z","iopub.status.idle":"2024-07-04T17:48:45.849193Z","shell.execute_reply.started":"2024-07-04T17:48:45.841989Z","shell.execute_reply":"2024-07-04T17:48:45.848048Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Here is an example of why squared error can not be applied for logistic regression and what the loss-function for it is\n### Squared-error cost funciton $J_w,_b \\vec{(x)} = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1} $","metadata":{}},{"cell_type":"code","source":"x_train = np.array([0., 1, 2, 3, 4, 5],dtype=np.longdouble)\ny_train = np.array([0,  0, 0, 1, 1, 1],dtype=np.longdouble)\nplt.scatter(x_train, y_train)\n# as shown, this data on being fit in the loss function does not give a convex curve which is crucial for getting the loss function","metadata":{"execution":{"iopub.status.busy":"2024-07-04T17:48:46.932579Z","iopub.execute_input":"2024-07-04T17:48:46.933327Z","iopub.status.idle":"2024-07-04T17:48:47.201629Z","shell.execute_reply.started":"2024-07-04T17:48:46.933289Z","shell.execute_reply":"2024-07-04T17:48:47.200426Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<matplotlib.collections.PathCollection at 0x7f01fd2e26e0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number. \n\n>**Definition Note:**   In this course, these definitions are used:  \n**Loss** is a measure of the difference of a single example to its target value while the  \n**Cost** is a measure of the losses over the training set\n\n\nThis is defined: \n* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n\n\\begin{equation}\n  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n  \\end{cases}\n\\end{equation}\n\n\n*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value.\n\n*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$ where function $g$ is the sigmoid function.\n\nThe defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or ($y=0$) and another for when the target is one ($y=1$). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target. Consider the curves below:","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Cost function for Logistic Regression\n\n## Cost function\n\nIn a previous lab, you developed the *logistic loss* function. Recall, loss is defined to apply to one example. Here you combine the losses to form the **cost**, which includes all the examples.\n\n\nRecall that for logistic regression, the cost function is of the form \n\n$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n\nwhere\n* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n\n    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n    \n*  where m is the number of training examples in the data set and:\n$$\n\\begin{align}\n  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)})\\tag{3} \\\\\n  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\\n  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5} \n\\end{align}\n$$\n ","metadata":{}},{"cell_type":"code","source":"X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n)\ny_train = np.array([0, 0, 0, 1, 1, 1])                                           #(m,)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T17:48:47.632412Z","iopub.execute_input":"2024-07-04T17:48:47.632803Z","iopub.status.idle":"2024-07-04T17:48:47.638866Z","shell.execute_reply.started":"2024-07-04T17:48:47.632770Z","shell.execute_reply":"2024-07-04T17:48:47.637665Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def compute_cost_logistic(X,y,w,b):\n    m = X.shape[0]\n    cost = 0.0\n    \n    for i in range(m):\n        z_i = np.dot(w,X) + b\n        f_wb_i = 1/(1+np.exp(-z_i))\n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-04T17:48:48.211511Z","iopub.execute_input":"2024-07-04T17:48:48.211864Z","iopub.status.idle":"2024-07-04T17:48:48.218308Z","shell.execute_reply.started":"2024-07-04T17:48:48.211835Z","shell.execute_reply":"2024-07-04T17:48:48.217062Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# New Lab: Gradient Descent with Logistic Regression","metadata":{}},{"cell_type":"code","source":"X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny_train = np.array([0, 0, 0, 1, 1, 1])","metadata":{"execution":{"iopub.status.busy":"2024-07-04T17:49:35.437497Z","iopub.execute_input":"2024-07-04T17:49:35.437875Z","iopub.status.idle":"2024-07-04T17:49:35.445255Z","shell.execute_reply.started":"2024-07-04T17:49:35.437846Z","shell.execute_reply":"2024-07-04T17:49:35.443959Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Gradient Descent\n\nRecall the gradient descent algorithm utilizes the gradient calculation:\n$$\\begin{align*}\n&\\text{repeat until convergence:} \\; \\lbrace \\\\\n&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\ \n&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n&\\rbrace\n\\end{align*}$$\n\nWhere each iteration performs simultaneous updates on $w_j$ for all $j$, where\n$$\\begin{align*}\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_{j}^{(i)} \\tag{2} \\\\\n\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)}) \\tag{3} \n\\end{align*}$$\n\n* m is the number of training examples in the data set      \n* $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target\n* For a logistic regression model  \n    $z = \\mathbf{w} \\cdot \\mathbf{x} + b$  \n    $f_{\\mathbf{w},b}(x) = g(z)$  \n    where $g(z)$ is the sigmoid function:  \n    $g(z) = \\frac{1}{1+e^{-z}}$   \n    \n","metadata":{}},{"cell_type":"code","source":"def sigmoid(x):\n    # returns sigmoid function for the given values\n    return 1 / (1 + np.exp(-x))","metadata":{"execution":{"iopub.status.busy":"2024-07-04T17:59:21.138423Z","iopub.execute_input":"2024-07-04T17:59:21.139408Z","iopub.status.idle":"2024-07-04T17:59:21.145728Z","shell.execute_reply.started":"2024-07-04T17:59:21.139372Z","shell.execute_reply":"2024-07-04T17:59:21.144503Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def compute_gradient_logistic(X,y,w,b):\n    m,n = X.shape()\n    dj_dw = np.zeroes((n,))\n    dj_db = 0\n    \n    for i in range(n):\n        f_wb_i = sigmoid(np.dot(X[i],w)+b)\n        err_i = f_wb_i - y[i];\n        for j in range(n):\n            dj_dw[j] = dj_dw[j] + err_i *X[i,j]\n        dj_db = dj_db + err_i\n    dj_dw = dj_dw/m\n    dj_db = dj_db/m\n    \n    return dj_db, dj_dw","metadata":{"execution":{"iopub.status.busy":"2024-07-04T18:02:38.224660Z","iopub.execute_input":"2024-07-04T18:02:38.225065Z","iopub.status.idle":"2024-07-04T18:02:38.233133Z","shell.execute_reply.started":"2024-07-04T18:02:38.225033Z","shell.execute_reply":"2024-07-04T18:02:38.231782Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"X_tmp = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\ny_tmp = np.array([0, 0, 0, 1, 1, 1])\nw_tmp = np.array([2.,3.])\nb_tmp = 1.\ndj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\nprint(f\"dj_db: {dj_db_tmp}\" )\nprint(f\"dj_dw: {dj_dw_tmp.tolist()}\" )","metadata":{"execution":{"iopub.status.busy":"2024-07-04T18:02:47.734982Z","iopub.execute_input":"2024-07-04T18:02:47.735462Z","iopub.status.idle":"2024-07-04T18:02:48.197080Z","shell.execute_reply.started":"2024-07-04T18:02:47.735425Z","shell.execute_reply":"2024-07-04T18:02:48.195386Z"},"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m w_tmp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m2.\u001b[39m,\u001b[38;5;241m3.\u001b[39m])\n\u001b[1;32m      4\u001b[0m b_tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.\u001b[39m\n\u001b[0;32m----> 5\u001b[0m dj_db_tmp, dj_dw_tmp \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_gradient_logistic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_tmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_tmp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdj_db: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdj_db_tmp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdj_dw: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdj_dw_tmp\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m )\n","Cell \u001b[0;32mIn[12], line 2\u001b[0m, in \u001b[0;36mcompute_gradient_logistic\u001b[0;34m(X, y, w, b)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_gradient_logistic\u001b[39m(X,y,w,b):\n\u001b[0;32m----> 2\u001b[0m     m,n \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     dj_dw \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeroes((n,))\n\u001b[1;32m      4\u001b[0m     dj_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"],"ename":"TypeError","evalue":"'tuple' object is not callable","output_type":"error"}]},{"cell_type":"code","source":"def gradient_descent(X, y, w_in, b_in, alpha, num_iters): \n    \"\"\"\n    Performs batch gradient descent\n    \n    Args:\n      X (ndarray (m,n)   : Data, m examples with n features\n      y (ndarray (m,))   : target values\n      w_in (ndarray (n,)): Initial values of model parameters  \n      b_in (scalar)      : Initial values of model parameter\n      alpha (float)      : Learning rate\n      num_iters (scalar) : number of iterations to run gradient descent\n      \n    Returns:\n      w (ndarray (n,))   : Updated values of parameters\n      b (scalar)         : Updated value of parameter \n    \"\"\"\n    # An array to store cost J and w's at each iteration primarily for graphing later\n    J_history = []\n    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n    b = b_in\n    \n    for i in range(num_iters):\n        # Calculate the gradient and update the parameters\n        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)   \n\n        # Update Parameters using w, b, alpha and gradient\n        w = w - alpha * dj_dw               \n        b = b - alpha * dj_db               \n      \n        # Save cost J at each iteration\n        if i<100000:      # prevent resource exhaustion \n            J_history.append( compute_cost_logistic(X, y, w, b) )\n\n        # Print cost every at intervals 10 times or as many iterations if < 10\n        if i% math.ceil(num_iters / 10) == 0:\n            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n        \n    return w, b, J_history         #return final w,b and J history for graphing\n","metadata":{},"execution_count":null,"outputs":[]}]}